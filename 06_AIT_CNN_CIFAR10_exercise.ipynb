{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lacykaltgr/ait-assessments/blob/main/06_AIT_CNN_CIFAR10_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW-EyI5Lxpqi"
      },
      "source": [
        "# Copyright\n",
        "\n",
        "<PRE>\n",
        "Copyright (c) 2023 Bálint Gyires-Tóth - All Rights Reserved\n",
        "You may use and modify this code for research and development purpuses.\n",
        "Using this code for educational purposes (self-paced or instructor led) without the permission of the author is prohibited.\n",
        "</PRE>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network exercise\n",
        "\n",
        "In this notebook, you will build a convolutional neural network, which offers superior results over the previous MLP solution. Additionally, you will perform a more detailed evaluation.  "
      ],
      "metadata": {
        "id": "3EX4cCa4WKJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports \n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "GdCMhy4HA2qd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data\n",
        "The dataset we will use is the well-known CIFAR10 (Canadian Institute For Advanced Research). Explore the details on the Keras website first: https://keras.io/api/datasets/cifar10/\n",
        "\n",
        "There isn't much difference between the data preprocessing part and the one we did before. The only difference is that we don't have to reshape images into vectors, but we are working with a 4 dimensional data structure, where the dimensions are: batch, X, Y, and color channels. \n",
        "\n",
        "In the event that anything is not clear regarding the data preparation part, please check the previous CIFAR10 notebook."
      ],
      "metadata": {
        "id": "yY9fztIeWIKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "bez7q0eiWQCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c9e12bb-c088-4f01-eac7-18e39999d44e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# it is in int8 format, the neural network requires float32\n",
        "X_train = X_train.astype(\"float32\")\n",
        "X_test = X_test.astype(\"float32\")"
      ],
      "metadata": {
        "id": "vukOcBM0XzSj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio  = 0.8\n",
        "train_length = X_train.shape[0]\n",
        "train_split  = int(train_ratio*train_length)\n",
        "X_valid, Y_valid = X_train[train_split:], Y_train[train_split:]\n",
        "X_train, Y_train = X_train[:train_split], Y_train[:train_split]"
      ],
      "metadata": {
        "id": "mRwuWxxIxLgN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = X_train.mean(axis=0)\n",
        "std  = X_train.std(axis=0)\n",
        "\n",
        "X_train = (X_train-mean)/std\n",
        "X_valid = (X_valid-mean)/std\n",
        "X_test  = (X_test-mean)/std"
      ],
      "metadata": {
        "id": "2ESLP1EIyB2J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classes = len(np.unique(Y_train))\n",
        "Y_train = to_categorical(Y_train, nb_classes)\n",
        "Y_valid = to_categorical(Y_valid, nb_classes)\n",
        "Y_test  = to_categorical(Y_test, nb_classes)"
      ],
      "metadata": {
        "id": "xwZ4OJ6I8lba"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shapes of the training, validation and test input data:\", X_train.shape, X_valid.shape, X_test.shape)\n",
        "print(\"Shapes of the training, validation and test output data:\", Y_train.shape, Y_valid.shape, Y_test.shape)\n",
        "print(\"Mean values of the training, validation and test input data:\", X_train.mean(), X_valid.mean(), X_test.mean())\n",
        "print(\"Standard deviation of the training, validation and test input data:\", X_train.std(), X_valid.std(), X_test.std())"
      ],
      "metadata": {
        "id": "O_jdJVcvX0vi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec215e35-2df5-438d-d5ec-1ace74afee6a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of the training, validation and test input data: (40000, 32, 32, 3) (10000, 32, 32, 3) (10000, 32, 32, 3)\n",
            "Shapes of the training, validation and test output data: (40000, 10) (10000, 10) (10000, 10)\n",
            "Mean values of the training, validation and test input data: 2.2788842e-09 0.0023437198 0.013331206\n",
            "Standard deviation of the training, validation and test input data: 1.0000023 0.9965626 0.99783075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ14oyZExpqj"
      },
      "source": [
        "# 2. Exercise: training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and train a convolutional neural network with the following requirements:\n",
        "* apply early stopping with patience=5 and monitor the validation accuracy, don't forget to load back the best weights after early stopping\n",
        "* the number of trainable parameters is lower than 200.000\n",
        "* the validation accuracy must be higher than 72%\n",
        "\n",
        "Hints:\n",
        "* you can always inspect the number of trainable parameters per layer with model.summary()\n",
        "* usually the most trainable parameters are between the last convolutional layer and the next dense layer -- so you should have a reasonable output size at the last convolutional layer\n",
        "* regularization helps (e.g. DropOut and/or BatchNormalization)\n",
        "* use the right activation function at the output layer and the corresponding loss function\n",
        "* for the chosen activation functions, please use the appropriate initialization method\n",
        "* you can match the dimension of the last conv/pooling layer to the next dense layer with e.g. Flatten()\n",
        "* ADAM optimizer is a reasonable choice \n",
        "* don't forget to turn on GPU support in Colab, otherwise, it would be quite slow to train the networks\n",
        "\n"
      ],
      "metadata": {
        "id": "j1tGVPdD-pp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True) # early stopping"
      ],
      "metadata": {
        "id": "5dBQxJHIHrbd"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape[1:]"
      ],
      "metadata": {
        "id": "tnUmUU_9zaj9",
        "outputId": "de547ba8-41e6-40ea-f1da-e6bef45af10b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model definition\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, (3,3), padding='same', activation='relu', input_shape=(X_train.shape[1:])))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.15))\n",
        "model.add(Conv2D(32, (2,2), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.15))\n",
        "model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.15))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(64, kernel_initializer=HeNormal()))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(32, kernel_initializer=HeNormal()))\n",
        "model.add(Dense(10, kernel_initializer=HeNormal(), activation='softmax'))\n",
        "\n",
        "# loss function and optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "PuxHOqLt-3Nd"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "5eomQw8uE9Ix",
        "outputId": "c37568da-9cd0-4497-aaeb-ff1f51d6c900",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_73 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_71 (MaxPoolin  (None, 16, 16, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_74 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_72 (MaxPoolin  (None, 8, 8, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_75 (Conv2D)          (None, 8, 8, 64)          36928     \n",
            "                                                                 \n",
            " max_pooling2d_73 (MaxPoolin  (None, 4, 4, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_21 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_32 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 32)                32800     \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 10)                330       \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93,546\n",
            "Trainable params: 91,498\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please, check the number of trainable paramers:"
      ],
      "metadata": {
        "id": "8tIbIIeTEI41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"#trainable parameters:\",model.count_params())"
      ],
      "metadata": {
        "id": "xdBAG7EJETlX",
        "outputId": "ba501fd2-0193-4526-b7ea-20c04b06df51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#trainable parameters: 134858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "network_history = model.fit(X_train, Y_train, \n",
        "                            validation_data=(X_valid, Y_valid), \n",
        "                            batch_size=64, \n",
        "                            epochs=200, \n",
        "                            callbacks=[es])"
      ],
      "metadata": {
        "id": "u8tFNCNVHyt9",
        "outputId": "269628e8-aac7-4f80-9aaf-2c2f703abcfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "625/625 [==============================] - 9s 9ms/step - loss: 1.8359 - val_loss: 1.5065\n",
            "Epoch 2/200\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 1.3168 - val_loss: 1.1709\n",
            "Epoch 3/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 1.1921 - val_loss: 1.0777\n",
            "Epoch 4/200\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 1.1086 - val_loss: 0.9715\n",
            "Epoch 5/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 1.0512 - val_loss: 0.9276\n",
            "Epoch 6/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 1.0059 - val_loss: 0.8926\n",
            "Epoch 7/200\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.9635 - val_loss: 0.8950\n",
            "Epoch 8/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.9324 - val_loss: 0.8335\n",
            "Epoch 9/200\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.9132 - val_loss: 0.8262\n",
            "Epoch 10/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.8920 - val_loss: 0.8256\n",
            "Epoch 11/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.8740 - val_loss: 0.7830\n",
            "Epoch 12/200\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.8588 - val_loss: 0.7762\n",
            "Epoch 13/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.8428 - val_loss: 0.7643\n",
            "Epoch 14/200\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.8210 - val_loss: 0.7313\n",
            "Epoch 15/200\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.8170 - val_loss: 0.7800\n",
            "Epoch 16/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.8094 - val_loss: 0.7676\n",
            "Epoch 17/200\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.7945 - val_loss: 0.7306\n",
            "Epoch 18/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.7877 - val_loss: 0.7774\n",
            "Epoch 19/200\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.7818 - val_loss: 0.8045\n",
            "Epoch 20/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.7712 - val_loss: 0.7341\n",
            "Epoch 21/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.7672 - val_loss: 0.7268\n",
            "Epoch 22/200\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.7623 - val_loss: 0.7530\n",
            "Epoch 23/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.7499 - val_loss: 0.7257\n",
            "Epoch 24/200\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.7460 - val_loss: 0.7252\n",
            "Epoch 25/200\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.7454 - val_loss: 0.7348\n",
            "Epoch 26/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.7390 - val_loss: 0.7182\n",
            "Epoch 27/200\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.7349 - val_loss: 0.7636\n",
            "Epoch 28/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.7320 - val_loss: 0.7325\n",
            "Epoch 29/200\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.7288 - val_loss: 0.6894\n",
            "Epoch 30/200\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.7160 - val_loss: 0.7082\n",
            "Epoch 31/200\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.7163 - val_loss: 0.7592\n",
            "Epoch 32/200\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.7221 - val_loss: 0.7352\n",
            "Epoch 33/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.7160 - val_loss: 0.7266\n",
            "Epoch 34/200\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.7151 - val_loss: 0.6878\n",
            "Epoch 35/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.7058 - val_loss: 0.6792\n",
            "Epoch 36/200\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.7023 - val_loss: 0.7212\n",
            "Epoch 37/200\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.6978 - val_loss: 0.6619\n",
            "Epoch 38/200\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.6999 - val_loss: 0.6841\n",
            "Epoch 39/200\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.6909 - val_loss: 0.7182\n",
            "Epoch 40/200\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.6880 - val_loss: 0.6943\n",
            "Epoch 41/200\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.6885 - val_loss: 0.6832\n",
            "Epoch 42/200\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.6904 - val_loss: 0.6995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Exercise: evaluation on test data\n",
        "You will evaluate the classification of the test data using the common metrics and the confusion matrix in this section. \n",
        "\n",
        "\n",
        "As a first stes, let's run the predictions and convert the predicted softmax outputs to dense labels. The one-hot encoded labels are also converted back to dense labels, as they are required for the evaluation functions."
      ],
      "metadata": {
        "id": "2FtKp-a2-A9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probas = model.predict(X_test)\n",
        "preds  = np.argmax(probas,axis=1)\n",
        "Y_test_dense = np.argmax(Y_test, axis=1) # get the original dense labels of the test data"
      ],
      "metadata": {
        "id": "EcWAqwlDMq3y",
        "outputId": "ee2c2743-f6a7-486e-868b-510553371315",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Exercise: Metrics\n",
        "please review the common classification metrics (accuracy, precision, recall, F1) for the complete model (not per class). \n",
        "\n",
        "Hints:\n",
        "* use macro averaging\n",
        "* you have to import the right functions from [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)\n"
      ],
      "metadata": {
        "id": "RWJ-PH_iMs9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "5sKm5zHSMZ_x"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", accuracy_score(Y_test_dense, preds))\n",
        "print(\"Precision:\", precision_score(Y_test_dense, preds, average='macro'))\n",
        "print(\"Recall:\", recall_score(Y_test_dense, preds, average='macro'))\n",
        "print(\"F1:\", f1_score(Y_test_dense, preds, average='macro'))"
      ],
      "metadata": {
        "id": "I3_9wgDREUna",
        "outputId": "61974829-215e-41f1-84aa-9ec0ab907de9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7708\n",
            "Precision: 0.7711782498540275\n",
            "Recall: 0.7707999999999999\n",
            "F1: 0.7697323966103156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3.2. Exercise: Confusion matrix\n",
        "Calculate the confusion matrix and display the results in a heatmap (with decimal values).\n",
        "\n",
        "Hints:\n",
        "* you have to do the imports this time too, e.g. from [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) and the [heatmap function](https://seaborn.pydata.org/generated/seaborn.heatmap.html) of [seaborn](https://seaborn.pydata.org/)\n",
        "* for displaying the decimal values, you have to turn annot=True and define fmt='d' in the heatmap function of seaborn"
      ],
      "metadata": {
        "id": "ECHguBFtGL7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "SIi2VzMBO3yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdqODx92Svw7"
      },
      "source": [
        "conf = <TODO>\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.heatmap(<TODO>)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}